import { v } from "convex/values";
import { action, mutation, query, internalMutation } from "./_generated/server";
import { api, internal } from "./_generated/api";
import { transcribeAudio, formatTimestamp, type DeepgramWord } from "./deepgram";
import { chunkTranscriptWithTimestamps, type TranscriptChunk } from "./semanticMatcher";
import { generateEmbeddings } from "./gemini";

// Get transcript from database (cached) or fetch from API if not available
export const getEpisodeTranscription = action({
  args: { episodeId: v.string() },
  handler: async (ctx, args): Promise<{
    episodeId: string;
    transcript: string | null;
    hasTranscript: boolean;
    updatedAt?: number;
    episodeTitle?: string;
    episodeDescription?: string;
    episodeAudio?: string;
    episodeDuration?: number;
  }> => {
    console.log(`Getting transcription for episode: ${args.episodeId}`);
    
    // First check if we have a cached transcription
    const cached: {
      episodeId: string;
      transcript: string | null;
      hasTranscript: boolean;
      updatedAt: number;
    } | null = await ctx.runQuery(api.transcriptions.getCachedTranscription, {
      episodeId: args.episodeId,
    });
    
    if (cached) {
      console.log(`Using cached transcription for episode ${args.episodeId}`);
      return cached;
    }
    
    console.log(`Fetching new transcription for episode ${args.episodeId}`);
    
    // Fetch from Listen Notes API
    const episodeData: {
      episodeId: string;
      transcript: string | null;
      hasTranscript: boolean;
      episodeTitle: string;
      episodeDescription: string;
      episodeAudio: string;
      episodeDuration: number;
    } = await ctx.runAction(api.podcasts.getEpisodeTranscript, {
      episodeId: args.episodeId,
    });
    
    // Cache the transcription result
    await ctx.runMutation(api.transcriptions.cacheTranscription, {
      episodeId: args.episodeId,
      transcript: episodeData.transcript || undefined,
      hasTranscript: episodeData.hasTranscript,
    });
    
    return episodeData;
  },
});

// Query to get cached transcription
export const getCachedTranscription = query({
  args: { episodeId: v.string() },
  handler: async (ctx, args): Promise<{
    episodeId: string;
    transcript: string | null;
    hasTranscript: boolean;
    updatedAt: number;
  } | null> => {
    const transcription = await ctx.db
      .query("transcriptions")
      .withIndex("by_episode_id", (q) => q.eq("episode_id", args.episodeId))
      .first();
    
    if (!transcription) {
      return null;
    }
    
    return {
      episodeId: transcription.episode_id,
      transcript: transcription.transcript || null,
      hasTranscript: transcription.has_transcript,
      updatedAt: transcription.updated_at,
    };
  },
});

// Mutation to cache transcription
export const cacheTranscription = mutation({
  args: {
    episodeId: v.string(),
    transcript: v.optional(v.string()),
    hasTranscript: v.boolean(),
  },
  handler: async (ctx, args) => {
    const now = Date.now();
    
    // Check if transcription already exists
    const existing = await ctx.db
      .query("transcriptions")
      .withIndex("by_episode_id", (q) => q.eq("episode_id", args.episodeId))
      .first();
    
    if (existing) {
      // Update existing transcription
      await ctx.db.patch(existing._id, {
        transcript: args.transcript,
        has_transcript: args.hasTranscript,
        updated_at: now,
      });
      
      console.log(`Updated cached transcription for episode ${args.episodeId}`);
    } else {
      // Create new transcription record
      await ctx.db.insert("transcriptions", {
        episode_id: args.episodeId,
        transcript: args.transcript,
        has_transcript: args.hasTranscript,
        created_at: now,
        updated_at: now,
        source: "listen_notes",
      });
      
      console.log(`Cached new transcription for episode ${args.episodeId}`);
    }
  },
});

// Query to check if transcription exists and is recent (for cache invalidation)
export const shouldRefreshTranscription = query({
  args: { episodeId: v.string() },
  handler: async (ctx, args) => {
    const transcription = await ctx.db
      .query("transcriptions")
      .withIndex("by_episode_id", (q) => q.eq("episode_id", args.episodeId))
      .first();
    
    if (!transcription) {
      return true; // No transcription cached, should fetch
    }
    
    // Check if transcription is older than 7 days (in case new transcripts become available)
    const sevenDaysAgo = Date.now() - 7 * 24 * 60 * 60 * 1000;
    const shouldRefresh = transcription.updated_at < sevenDaysAgo;
    
    return shouldRefresh;
  },
});

// Action to transcribe episode using Deepgram
export const transcribeEpisodeWithDeepgram = action({
  args: { 
    episodeId: v.string(),
    audioUrl: v.string(),
    forceRetranscribe: v.optional(v.boolean())
  },
  handler: async (ctx, args): Promise<{
    success: boolean;
    transcript?: string;
    hasTimestamps: boolean;
    wordCount?: number;
    confidence?: number;
    duration?: number;
    error?: string;
  }> => {
    console.log(`üéôÔ∏è Starting Deepgram transcription for episode: ${args.episodeId}`);
    
    try {
      // Check if we already have a Deepgram transcription (unless forcing retranscribe)
      if (!args.forceRetranscribe) {
        const existing = await ctx.runQuery(api.transcriptions.getCachedTranscriptionWithTimestamps, {
          episodeId: args.episodeId,
        });
        
        if (existing && existing.source === "deepgram" && existing.hasTimestamps) {
          console.log(`‚úÖ Using existing Deepgram transcription for episode ${args.episodeId}`);
          return {
            success: true,
            transcript: existing.transcript || undefined,
            hasTimestamps: true,
            wordCount: existing.wordCount,
            confidence: existing.confidence,
            duration: existing.duration,
          };
        }
      }

      // Validate audio URL
      if (!args.audioUrl || !args.audioUrl.startsWith('http')) {
        throw new Error("Invalid audio URL provided");
      }

      // Transcribe with Deepgram
      const deepgramResult = await transcribeAudio(args.audioUrl);
      
      if (!deepgramResult?.transcript?.transcript) {
        throw new Error("No transcript received from Deepgram");
      }

      // Handle Convex array size limit (8192 items max) before storing
      const MAX_TIMESTAMPS = 8000; // Leave some buffer
      let optimizedWords = deepgramResult.transcript.words;
      
      if (deepgramResult.transcript.words.length > MAX_TIMESTAMPS) {
        console.log(`‚ö†Ô∏è Episode ${args.episodeId} has ${deepgramResult.transcript.words.length} word timestamps, reducing to ${MAX_TIMESTAMPS} for storage`);
        
        // Sample timestamps evenly across the episode to maintain coverage
        const step = Math.floor(deepgramResult.transcript.words.length / MAX_TIMESTAMPS);
        optimizedWords = deepgramResult.transcript.words.filter((_, index) => index % step === 0).slice(0, MAX_TIMESTAMPS);
        
        console.log(`‚úÖ Optimized to ${optimizedWords.length} timestamps (every ${step}th word)`);
      }

      // Store transcription with optimized timestamps
      await ctx.runMutation(internal.transcriptions.storeDeepgramTranscription, {
        episodeId: args.episodeId,
        transcript: deepgramResult.transcript.transcript,
        wordTimestamps: optimizedWords.map(word => ({
          word: word.word,
          start: word.start,
          end: word.end,
          confidence: word.confidence,
        })),
        confidence: deepgramResult.transcript.confidence,
        duration: deepgramResult.metadata.duration,
      });

      console.log(`‚úÖ Deepgram transcription completed for episode ${args.episodeId}: ${deepgramResult.transcript.words.length} original words, ${optimizedWords.length} stored`);

      // NEW: Generate semantic chunks and embeddings for improved timestamp matching
      try {
        console.log(`üß† GENERATING SEMANTIC CHUNKS AND EMBEDDINGS for episode ${args.episodeId}`);

        // Create semantic chunks from the transcript
        const chunks = chunkTranscriptWithTimestamps(
          deepgramResult.transcript.transcript,
          optimizedWords
        );

        // Generate embeddings for all chunks
        const chunkTexts = chunks.map(chunk => chunk.text);
        const embeddings = await generateEmbeddings(chunkTexts);

        // Combine chunks with their embeddings
        const chunksWithEmbeddings = chunks.map((chunk, index) => ({
          id: chunk.id,
          text: chunk.text,
          startTime: chunk.startTime,
          endTime: chunk.endTime,
          embedding: embeddings[index],
        }));

        // Update the stored transcription with semantic chunks
        await ctx.runMutation(internal.transcriptions.updateTranscriptionChunks, {
          episodeId: args.episodeId,
          chunks: chunksWithEmbeddings,
          embeddingVersion: "text-embedding-004",
          chunksGenerated: Date.now(),
        });

        console.log(`üéØ EMBEDDINGS COMPLETE: Generated ${chunksWithEmbeddings.length} semantic chunks with embeddings`);

      } catch (embeddingError) {
        console.error(`‚ö†Ô∏è EMBEDDING GENERATION FAILED for episode ${args.episodeId}:`, embeddingError);
        // Continue without embeddings - semantic matching will fall back to lexical
      }

      return {
        success: true,
        transcript: deepgramResult.transcript.transcript,
        hasTimestamps: true,
        wordCount: optimizedWords.length,
        confidence: deepgramResult.transcript.confidence,
        duration: deepgramResult.metadata.duration,
      };

    } catch (error: any) {
      console.error(`‚ùå Deepgram transcription failed for episode ${args.episodeId}:`, error);
      
      return {
        success: false,
        hasTimestamps: false,
        error: error.message || "Unknown transcription error",
      };
    }
  },
});

// Internal mutation to update transcription with semantic chunks
export const updateTranscriptionChunks = internalMutation({
  args: {
    episodeId: v.string(),
    chunks: v.array(v.object({
      id: v.string(),
      text: v.string(),
      startTime: v.number(),
      endTime: v.number(),
      embedding: v.array(v.number()),
    })),
    embeddingVersion: v.string(),
    chunksGenerated: v.number(),
  },
  handler: async (ctx, args) => {
    console.log(`üìù UPDATING TRANSCRIPTION CHUNKS for episode ${args.episodeId}: ${args.chunks.length} chunks`);

    // Find the existing transcription record
    const existingTranscription = await ctx.db
      .query("transcriptions")
      .withIndex("by_episode_id", (q) => q.eq("episode_id", args.episodeId))
      .first();

    if (!existingTranscription) {
      console.error(`‚ùå No transcription found for episode ${args.episodeId}`);
      throw new Error(`No transcription found for episode ${args.episodeId}`);
    }

    // Update with semantic chunks
    await ctx.db.patch(existingTranscription._id, {
      chunks: args.chunks,
      embeddingVersion: args.embeddingVersion,
      chunksGenerated: args.chunksGenerated,
      updated_at: Date.now(),
    });

    console.log(`‚úÖ SEMANTIC CHUNKS STORED for episode ${args.episodeId}`);
  },
});

// Query to get cached transcription with timestamps
export const getCachedTranscriptionWithTimestamps = query({
  args: { episodeId: v.string() },
  handler: async (ctx, args): Promise<{
    episodeId: string;
    transcript: string | null;
    hasTranscript: boolean;
    hasTimestamps: boolean;
    wordTimestamps?: DeepgramWord[];
    confidence?: number;
    duration?: number;
    wordCount?: number;
    source?: string;
    updatedAt: number;
  } | null> => {
    const transcription = await ctx.db
      .query("transcriptions")
      .withIndex("by_episode_id", (q) => q.eq("episode_id", args.episodeId))
      .first();
    
    if (!transcription) {
      return null;
    }
    
    return {
      episodeId: transcription.episode_id,
      transcript: transcription.transcript || null,
      hasTranscript: transcription.has_transcript,
      hasTimestamps: !!transcription.word_timestamps && transcription.word_timestamps.length > 0,
      wordTimestamps: transcription.word_timestamps || undefined,
      confidence: transcription.confidence,
      duration: transcription.duration,
      wordCount: transcription.word_timestamps?.length || 0,
      source: transcription.source,
      updatedAt: transcription.updated_at,
    };
  },
});

// Internal mutation to store Deepgram transcription
export const storeDeepgramTranscription = internalMutation({
  args: {
    episodeId: v.string(),
    transcript: v.string(),
    wordTimestamps: v.array(v.object({
      word: v.string(),
      start: v.number(),
      end: v.number(),
      confidence: v.number(),
    })),
    confidence: v.number(),
    duration: v.number(),
  },
  handler: async (ctx, args) => {
    const now = Date.now();
    
    // Check if transcription already exists
    const existing = await ctx.db
      .query("transcriptions")
      .withIndex("by_episode_id", (q) => q.eq("episode_id", args.episodeId))
      .first();
    
    const transcriptionData = {
      transcript: args.transcript,
      has_transcript: true,
      word_timestamps: args.wordTimestamps,
      confidence: args.confidence,
      duration: args.duration,
      updated_at: now,
      source: "deepgram",
    };
    
    if (existing) {
      // Update existing transcription
      await ctx.db.patch(existing._id, transcriptionData);
      console.log(`‚úÖ Updated Deepgram transcription for episode ${args.episodeId}`);
    } else {
      // Create new transcription record
      await ctx.db.insert("transcriptions", {
        episode_id: args.episodeId,
        created_at: now,
        ...transcriptionData,
      });
      console.log(`‚úÖ Stored new Deepgram transcription for episode ${args.episodeId}`);
    }
  },
});